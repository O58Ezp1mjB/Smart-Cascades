{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "\n",
    "# 'TSP' is our paper project code\n",
    "from pytorch_tabnet.TSP_implement import TSP_Loss\n",
    "\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Settings\n",
    "- `data_path_fast` and `data_path_expensive` are datasets for an estimator that can be replaced. Please select the dataset for a specific fast/expensive model in `./dataset_estimator/CIFAR100`.\n",
    "\n",
    "- `lambda_CDC_weight` is the `lambda` in the paper.\n",
    "\n",
    "- The trained weight will be stored in `./estimator_weights/weights_{fast_model}_{expensive_model}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.abspath('.')\n",
    "\n",
    "# dataset path\n",
    "data_path_fast= os.path.join(root,'dataset_estimator','CIFAR100','resnet18_dataset.csv')\n",
    "data_path_expensive=os.path.join(root,'dataset_estimator','CIFAR100','resnet101_dataset.csv')\n",
    "\n",
    "# setting\n",
    "dataset_split_type = 'split_by_json' # 'split_by_json' or 'split_by_random'\n",
    "json_for_estimator = os.path.join(root,'seeds_json','json_split_for_estimator_1.json')\n",
    "\n",
    "# The metric is used for early stopping\n",
    "eval_metric=['AIR'] # 'AIR' or 'TSP-Loss-Overall'\n",
    "\n",
    "learning_rate = 2e-3\n",
    "max_epochs = 200\n",
    "\n",
    "lambda_CDC_weight = 0.2 # 0.1, 0.15 or 0.2\n",
    "Loss_Base_weight = 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weight name\n",
    "if lambda_CDC_weight != 0:\n",
    "    loss_type = \"L-Overall\"\n",
    "else: \n",
    "    loss_type = \"L-Base\"\n",
    "\n",
    "if eval_metric[-1] == 'TSP-Loss-Overall':\n",
    "    metric_name = \"Loss\"\n",
    "else: \n",
    "    metric_name = \"AIR\"\n",
    "\n",
    "seed_id = json_for_estimator.split('_')[-1].split('.')[0]\n",
    "\n",
    "shallow_name = os.path.split(data_path_fast.split('_dataset')[0])[-1]\n",
    "deep_name = os.path.split(data_path_expensive.split('_dataset')[0])[-1]\n",
    "\n",
    "saving_weight_name = os.path.join(root,'estimator_weights',f'weights_{shallow_name}_{deep_name}',f'TabNet_{loss_type}_{metric_name}_Seed_{seed_id}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 103)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_in_dataset</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correct_cls_index</th>\n",
       "      <th>prob_index_0</th>\n",
       "      <th>prob_index_1</th>\n",
       "      <th>prob_index_2</th>\n",
       "      <th>prob_index_3</th>\n",
       "      <th>prob_index_4</th>\n",
       "      <th>prob_index_5</th>\n",
       "      <th>prob_index_6</th>\n",
       "      <th>...</th>\n",
       "      <th>prob_index_90</th>\n",
       "      <th>prob_index_91</th>\n",
       "      <th>prob_index_92</th>\n",
       "      <th>prob_index_93</th>\n",
       "      <th>prob_index_94</th>\n",
       "      <th>prob_index_95</th>\n",
       "      <th>prob_index_96</th>\n",
       "      <th>prob_index_97</th>\n",
       "      <th>prob_index_98</th>\n",
       "      <th>prob_index_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013671</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.005671</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.001430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47652</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7641</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.938051</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29229</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13143</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.055704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_in_dataset  correctness  correct_cls_index  prob_index_0  \\\n",
       "0              1384            1                 12      0.003795   \n",
       "1             47652            0                 52      0.000691   \n",
       "2              7641            1                 94      0.000874   \n",
       "3             29229            1                 82      0.000273   \n",
       "4             13143            1                 78      0.000508   \n",
       "\n",
       "   prob_index_1  prob_index_2  prob_index_3  prob_index_4  prob_index_5  \\\n",
       "0      0.001102      0.004457      0.003634      0.002040      0.008378   \n",
       "1      0.000299      0.003798      0.000804      0.000154      0.000505   \n",
       "2      0.000264      0.000128      0.000102      0.000478      0.000159   \n",
       "3      0.000217      0.000216      0.000362      0.000523      0.000812   \n",
       "4      0.000934      0.001392      0.004842      0.000776      0.001373   \n",
       "\n",
       "   prob_index_6  ...  prob_index_90  prob_index_91  prob_index_92  \\\n",
       "0      0.000414  ...       0.013671       0.000946       0.001571   \n",
       "1      0.000129  ...       0.000485       0.000235       0.000484   \n",
       "2      0.002260  ...       0.000117       0.000072       0.001296   \n",
       "3      0.011486  ...       0.000503       0.000302       0.005693   \n",
       "4      0.000226  ...       0.002749       0.000832       0.001555   \n",
       "\n",
       "   prob_index_93  prob_index_94  prob_index_95  prob_index_96  prob_index_97  \\\n",
       "0       0.000334       0.001054       0.005671       0.001071       0.002967   \n",
       "1       0.000238       0.000952       0.000562       0.009734       0.000484   \n",
       "2       0.001115       0.938051       0.000655       0.000327       0.000278   \n",
       "3       0.000231       0.000720       0.000559       0.000665       0.000743   \n",
       "4       0.000562       0.001098       0.003118       0.002139       0.002678   \n",
       "\n",
       "   prob_index_98  prob_index_99  \n",
       "0       0.003104       0.001430  \n",
       "1       0.000404       0.000460  \n",
       "2       0.000118       0.000482  \n",
       "3       0.001218       0.000375  \n",
       "4       0.001215       0.055704  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shallow = pd.read_csv(data_path_fast,header=0)\n",
    "print(data_shallow.shape)\n",
    "\n",
    "data_shallow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 103)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_in_dataset</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correct_cls_index</th>\n",
       "      <th>prob_index_0</th>\n",
       "      <th>prob_index_1</th>\n",
       "      <th>prob_index_2</th>\n",
       "      <th>prob_index_3</th>\n",
       "      <th>prob_index_4</th>\n",
       "      <th>prob_index_5</th>\n",
       "      <th>prob_index_6</th>\n",
       "      <th>...</th>\n",
       "      <th>prob_index_90</th>\n",
       "      <th>prob_index_91</th>\n",
       "      <th>prob_index_92</th>\n",
       "      <th>prob_index_93</th>\n",
       "      <th>prob_index_94</th>\n",
       "      <th>prob_index_95</th>\n",
       "      <th>prob_index_96</th>\n",
       "      <th>prob_index_97</th>\n",
       "      <th>prob_index_98</th>\n",
       "      <th>prob_index_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47652</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.050321</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7641</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.919552</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29229</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13143</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.030120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_in_dataset  correctness  correct_cls_index  prob_index_0  \\\n",
       "0              1384            1                 12      0.000743   \n",
       "1             47652            0                 52      0.001202   \n",
       "2              7641            1                 94      0.000882   \n",
       "3             29229            1                 82      0.000366   \n",
       "4             13143            1                 78      0.000386   \n",
       "\n",
       "   prob_index_1  prob_index_2  prob_index_3  prob_index_4  prob_index_5  \\\n",
       "0      0.000413      0.000473      0.000702      0.001246      0.005389   \n",
       "1      0.000653      0.000856      0.001213      0.000425      0.000925   \n",
       "2      0.000636      0.000793      0.000435      0.000892      0.003257   \n",
       "3      0.000351      0.000187      0.000696      0.000644      0.000296   \n",
       "4      0.000672      0.000813      0.000718      0.000666      0.000286   \n",
       "\n",
       "   prob_index_6  ...  prob_index_90  prob_index_91  prob_index_92  \\\n",
       "0      0.000932  ...       0.000386       0.000754       0.001024   \n",
       "1      0.000809  ...       0.000573       0.000655       0.000943   \n",
       "2      0.002244  ...       0.000737       0.000604       0.000930   \n",
       "3      0.001348  ...       0.000401       0.000216       0.006174   \n",
       "4      0.000410  ...       0.000838       0.001803       0.001267   \n",
       "\n",
       "   prob_index_93  prob_index_94  prob_index_95  prob_index_96  prob_index_97  \\\n",
       "0       0.001731       0.000540       0.001054       0.000727       0.001175   \n",
       "1       0.000776       0.000982       0.001496       0.050321       0.001202   \n",
       "2       0.000844       0.919552       0.001065       0.000744       0.000465   \n",
       "3       0.000173       0.000313       0.000589       0.000720       0.000626   \n",
       "4       0.000378       0.000859       0.001674       0.001201       0.000477   \n",
       "\n",
       "   prob_index_98  prob_index_99  \n",
       "0       0.001221       0.000555  \n",
       "1       0.001111       0.001907  \n",
       "2       0.000798       0.000858  \n",
       "3       0.000435       0.000155  \n",
       "4       0.000381       0.030120  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_deep = pd.read_csv(data_path_expensive,header=0)\n",
    "print(data_deep.shape)\n",
    "\n",
    "data_deep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       True\n",
      "1       True\n",
      "2       True\n",
      "3       True\n",
      "4       True\n",
      "        ... \n",
      "9995    True\n",
      "9996    True\n",
      "9997    True\n",
      "9998    True\n",
      "9999    True\n",
      "Name: index_in_dataset, Length: 10000, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# check filename\n",
    "print(data_shallow.iloc[:,0]==data_deep.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 203)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_index_0</th>\n",
       "      <th>prob_index_1</th>\n",
       "      <th>prob_index_2</th>\n",
       "      <th>prob_index_3</th>\n",
       "      <th>prob_index_4</th>\n",
       "      <th>prob_index_5</th>\n",
       "      <th>prob_index_6</th>\n",
       "      <th>prob_index_7</th>\n",
       "      <th>prob_index_8</th>\n",
       "      <th>prob_index_9</th>\n",
       "      <th>...</th>\n",
       "      <th>prob_index_93</th>\n",
       "      <th>prob_index_94</th>\n",
       "      <th>prob_index_95</th>\n",
       "      <th>prob_index_96</th>\n",
       "      <th>prob_index_97</th>\n",
       "      <th>prob_index_98</th>\n",
       "      <th>prob_index_99</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correct_cls_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.050321</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.919552</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   prob_index_0  prob_index_1  prob_index_2  prob_index_3  prob_index_4  \\\n",
       "0      0.003795      0.001102      0.004457      0.003634      0.002040   \n",
       "1      0.000691      0.000299      0.003798      0.000804      0.000154   \n",
       "2      0.000874      0.000264      0.000128      0.000102      0.000478   \n",
       "3      0.000273      0.000217      0.000216      0.000362      0.000523   \n",
       "4      0.000508      0.000934      0.001392      0.004842      0.000776   \n",
       "\n",
       "   prob_index_5  prob_index_6  prob_index_7  prob_index_8  prob_index_9  ...  \\\n",
       "0      0.008378      0.000414      0.002539      0.006601      0.001594  ...   \n",
       "1      0.000505      0.000129      0.000344      0.000887      0.001053  ...   \n",
       "2      0.000159      0.002260      0.001749      0.000146      0.001091  ...   \n",
       "3      0.000812      0.011486      0.000421      0.000445      0.000648  ...   \n",
       "4      0.001373      0.000226      0.001912      0.001828      0.000898  ...   \n",
       "\n",
       "   prob_index_93  prob_index_94  prob_index_95  prob_index_96  prob_index_97  \\\n",
       "0       0.001731       0.000540       0.001054       0.000727       0.001175   \n",
       "1       0.000776       0.000982       0.001496       0.050321       0.001202   \n",
       "2       0.000844       0.919552       0.001065       0.000744       0.000465   \n",
       "3       0.000173       0.000313       0.000589       0.000720       0.000626   \n",
       "4       0.000378       0.000859       0.001674       0.001201       0.000477   \n",
       "\n",
       "   prob_index_98  prob_index_99  correctness  correctness  correct_cls_index  \n",
       "0       0.001221       0.000555            1            1                 12  \n",
       "1       0.001111       0.001907            0            0                 52  \n",
       "2       0.000798       0.000858            1            1                 94  \n",
       "3       0.000435       0.000155            1            1                 82  \n",
       "4       0.000381       0.030120            1            1                 78  \n",
       "\n",
       "[5 rows x 203 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traning_data=pd.concat([data_shallow.iloc[:,3:],data_deep.iloc[:,3:],data_shallow.iloc[:,1],data_deep.iloc[:,1],data_shallow.iloc[:,2]],axis=1)\n",
    "print(traning_data.shape)\n",
    "traning_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 200) (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "if dataset_split_type == 'split_by_random':  \n",
    "    training_data_dim=traning_data.shape[1]-3\n",
    "    \n",
    "    traning_data[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(traning_data.shape[0],))\n",
    "\n",
    "    train_indices = traning_data[traning_data.Set==\"train\"].index\n",
    "    valid_indices = traning_data[traning_data.Set==\"valid\"].index\n",
    "    test_indices = traning_data[traning_data.Set==\"test\"].index\n",
    "\n",
    "    X_train = traning_data.iloc[:,:training_data_dim].values[train_indices]\n",
    "    y_train = traning_data.iloc[:,training_data_dim:-1].values[train_indices]#.reshape(-1, 3)\n",
    "\n",
    "    X_valid = traning_data.iloc[:,:training_data_dim].values[valid_indices]\n",
    "    y_valid = traning_data.iloc[:,training_data_dim:-1].values[valid_indices]#.reshape(-1, 3)\n",
    "\n",
    "    X_test = traning_data.iloc[:,:training_data_dim].values[test_indices]\n",
    "    y_test = traning_data.iloc[:,training_data_dim:-1].values[test_indices]#.reshape(-1, 3)\n",
    "    print(X_valid.shape,y_valid.shape)\n",
    "\n",
    "elif dataset_split_type == 'split_by_json':\n",
    "    training_data_dim = traning_data.shape[1]-3\n",
    "\n",
    "    # Add index to the last column\n",
    "    traning_data = pd.concat([traning_data,data_shallow.iloc[:,0]],axis=1)\n",
    "\n",
    "    with open(json_for_estimator) as f:\n",
    "        estimator_split = json.load(f)\n",
    "\n",
    "    X_train = traning_data[traning_data['index_in_dataset'].isin(estimator_split['index_train'])].iloc[:,:training_data_dim].values\n",
    "    y_train = traning_data[traning_data['index_in_dataset'].isin(estimator_split['index_train'])].iloc[:,training_data_dim:-1].values\n",
    "\n",
    "    X_valid = traning_data[traning_data['index_in_dataset'].isin(estimator_split['index_val'])].iloc[:,:training_data_dim].values\n",
    "    y_valid = traning_data[traning_data['index_in_dataset'].isin(estimator_split['index_val'])].iloc[:,training_data_dim:-1].values \n",
    "\n",
    "    print(X_valid.shape, y_valid.shape) \n",
    "    \n",
    "else:\n",
    "    raise Exception('please provide dataset_split_type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yaoching\\Desktop\\ICCV_Smart_cascade_code\\pytorch_tabnet\\abstract_model.py:77: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetRegressor(optimizer_params=dict(lr=learning_rate),scheduler_fn=torch.optim.lr_scheduler.StepLR,scheduler_params={\"gamma\": 0.9, \"step_size\": 5})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP_TabNet!\n",
      "epoch 0  | loss: 2.61202 | train_AIR: 0.64139 | valid_AIR: 0.62844 |  0:00:03s\n",
      "epoch 1  | loss: 1.97001 | train_AIR: 0.6779  | valid_AIR: 0.58486 |  0:00:04s\n",
      "epoch 2  | loss: 1.52769 | train_AIR: 0.73315 | valid_AIR: 0.68578 |  0:00:05s\n",
      "epoch 3  | loss: 1.14739 | train_AIR: 0.71723 | valid_AIR: 0.70872 |  0:00:07s\n",
      "epoch 4  | loss: 0.91326 | train_AIR: 0.71255 | valid_AIR: 0.73165 |  0:00:08s\n",
      "epoch 5  | loss: 0.94872 | train_AIR: 0.76873 | valid_AIR: 0.74541 |  0:00:09s\n",
      "epoch 6  | loss: 0.7496  | train_AIR: 0.7706  | valid_AIR: 0.75229 |  0:00:11s\n",
      "epoch 7  | loss: 0.66606 | train_AIR: 0.77154 | valid_AIR: 0.77294 |  0:00:12s\n",
      "epoch 8  | loss: 0.56893 | train_AIR: 0.76592 | valid_AIR: 0.74083 |  0:00:13s\n",
      "epoch 9  | loss: 0.51383 | train_AIR: 0.76124 | valid_AIR: 0.71101 |  0:00:15s\n",
      "epoch 10 | loss: 0.47333 | train_AIR: 0.72378 | valid_AIR: 0.68807 |  0:00:16s\n",
      "epoch 11 | loss: 0.46596 | train_AIR: 0.71536 | valid_AIR: 0.66284 |  0:00:17s\n",
      "epoch 12 | loss: 0.42913 | train_AIR: 0.68165 | valid_AIR: 0.62844 |  0:00:19s\n",
      "epoch 13 | loss: 0.40918 | train_AIR: 0.68258 | valid_AIR: 0.63073 |  0:00:20s\n",
      "epoch 14 | loss: 0.40559 | train_AIR: 0.64513 | valid_AIR: 0.61697 |  0:00:21s\n",
      "epoch 15 | loss: 0.37893 | train_AIR: 0.67416 | valid_AIR: 0.63303 |  0:00:23s\n",
      "epoch 16 | loss: 0.37306 | train_AIR: 0.73127 | valid_AIR: 0.70183 |  0:00:24s\n",
      "epoch 17 | loss: 0.35036 | train_AIR: 0.72566 | valid_AIR: 0.69725 |  0:00:25s\n",
      "epoch 18 | loss: 0.34038 | train_AIR: 0.72472 | valid_AIR: 0.70183 |  0:00:26s\n",
      "epoch 19 | loss: 0.32998 | train_AIR: 0.72378 | valid_AIR: 0.69037 |  0:00:28s\n",
      "epoch 20 | loss: 0.323   | train_AIR: 0.70225 | valid_AIR: 0.63991 |  0:00:29s\n",
      "epoch 21 | loss: 0.3123  | train_AIR: 0.63951 | valid_AIR: 0.58486 |  0:00:30s\n",
      "epoch 22 | loss: 0.29662 | train_AIR: 0.68539 | valid_AIR: 0.63073 |  0:00:32s\n",
      "epoch 23 | loss: 0.29564 | train_AIR: 0.6985  | valid_AIR: 0.66972 |  0:00:33s\n",
      "epoch 24 | loss: 0.28752 | train_AIR: 0.69757 | valid_AIR: 0.68807 |  0:00:34s\n",
      "epoch 25 | loss: 0.28411 | train_AIR: 0.72285 | valid_AIR: 0.69954 |  0:00:36s\n",
      "epoch 26 | loss: 0.27765 | train_AIR: 0.70318 | valid_AIR: 0.69266 |  0:00:37s\n",
      "epoch 27 | loss: 0.26616 | train_AIR: 0.71629 | valid_AIR: 0.69495 |  0:00:38s\n",
      "epoch 28 | loss: 0.2609  | train_AIR: 0.73221 | valid_AIR: 0.69725 |  0:00:40s\n",
      "epoch 29 | loss: 0.25753 | train_AIR: 0.72659 | valid_AIR: 0.69266 |  0:00:41s\n",
      "epoch 30 | loss: 0.25906 | train_AIR: 0.73876 | valid_AIR: 0.7156  |  0:00:42s\n",
      "epoch 31 | loss: 0.25758 | train_AIR: 0.74251 | valid_AIR: 0.71101 |  0:00:44s\n",
      "epoch 32 | loss: 0.24831 | train_AIR: 0.74345 | valid_AIR: 0.69495 |  0:00:45s\n",
      "epoch 33 | loss: 0.24647 | train_AIR: 0.74157 | valid_AIR: 0.7156  |  0:00:46s\n",
      "epoch 34 | loss: 0.24521 | train_AIR: 0.74251 | valid_AIR: 0.71789 |  0:00:48s\n",
      "epoch 35 | loss: 0.23989 | train_AIR: 0.76404 | valid_AIR: 0.72706 |  0:00:49s\n",
      "epoch 36 | loss: 0.2406  | train_AIR: 0.75281 | valid_AIR: 0.74312 |  0:00:50s\n",
      "epoch 37 | loss: 0.23725 | train_AIR: 0.75936 | valid_AIR: 0.75    |  0:00:52s\n",
      "epoch 38 | loss: 0.23708 | train_AIR: 0.75655 | valid_AIR: 0.74771 |  0:00:53s\n",
      "epoch 39 | loss: 0.23366 | train_AIR: 0.7603  | valid_AIR: 0.74771 |  0:00:54s\n",
      "epoch 40 | loss: 0.23273 | train_AIR: 0.75843 | valid_AIR: 0.75917 |  0:00:56s\n",
      "epoch 41 | loss: 0.23316 | train_AIR: 0.76124 | valid_AIR: 0.76606 |  0:00:57s\n",
      "epoch 42 | loss: 0.23116 | train_AIR: 0.75468 | valid_AIR: 0.76376 |  0:00:58s\n",
      "epoch 43 | loss: 0.2313  | train_AIR: 0.76124 | valid_AIR: 0.73624 |  0:01:00s\n",
      "epoch 44 | loss: 0.22907 | train_AIR: 0.75281 | valid_AIR: 0.74541 |  0:01:01s\n",
      "epoch 45 | loss: 0.22776 | train_AIR: 0.76217 | valid_AIR: 0.74771 |  0:01:02s\n",
      "epoch 46 | loss: 0.22668 | train_AIR: 0.75936 | valid_AIR: 0.73394 |  0:01:04s\n",
      "epoch 47 | loss: 0.22358 | train_AIR: 0.76966 | valid_AIR: 0.74312 |  0:01:05s\n",
      "epoch 48 | loss: 0.2267  | train_AIR: 0.76498 | valid_AIR: 0.75917 |  0:01:06s\n",
      "epoch 49 | loss: 0.22454 | train_AIR: 0.75562 | valid_AIR: 0.76376 |  0:01:08s\n",
      "epoch 50 | loss: 0.22252 | train_AIR: 0.75562 | valid_AIR: 0.75459 |  0:01:09s\n",
      "epoch 51 | loss: 0.22091 | train_AIR: 0.75749 | valid_AIR: 0.74771 |  0:01:10s\n",
      "epoch 52 | loss: 0.22196 | train_AIR: 0.76966 | valid_AIR: 0.75917 |  0:01:12s\n",
      "epoch 53 | loss: 0.22093 | train_AIR: 0.7603  | valid_AIR: 0.75688 |  0:01:13s\n",
      "epoch 54 | loss: 0.22159 | train_AIR: 0.75843 | valid_AIR: 0.74541 |  0:01:14s\n",
      "epoch 55 | loss: 0.22125 | train_AIR: 0.76498 | valid_AIR: 0.75688 |  0:01:15s\n",
      "epoch 56 | loss: 0.22118 | train_AIR: 0.75843 | valid_AIR: 0.77523 |  0:01:17s\n",
      "epoch 57 | loss: 0.21948 | train_AIR: 0.75936 | valid_AIR: 0.76835 |  0:01:18s\n",
      "epoch 58 | loss: 0.21759 | train_AIR: 0.76404 | valid_AIR: 0.75    |  0:01:19s\n",
      "epoch 59 | loss: 0.21772 | train_AIR: 0.75749 | valid_AIR: 0.75459 |  0:01:21s\n",
      "epoch 60 | loss: 0.21702 | train_AIR: 0.75655 | valid_AIR: 0.76606 |  0:01:22s\n",
      "epoch 61 | loss: 0.21649 | train_AIR: 0.75562 | valid_AIR: 0.76147 |  0:01:23s\n",
      "epoch 62 | loss: 0.215   | train_AIR: 0.75843 | valid_AIR: 0.76147 |  0:01:25s\n",
      "epoch 63 | loss: 0.21445 | train_AIR: 0.76311 | valid_AIR: 0.76147 |  0:01:26s\n",
      "epoch 64 | loss: 0.21391 | train_AIR: 0.76779 | valid_AIR: 0.76376 |  0:01:27s\n",
      "epoch 65 | loss: 0.21361 | train_AIR: 0.76779 | valid_AIR: 0.76147 |  0:01:29s\n",
      "epoch 66 | loss: 0.2133  | train_AIR: 0.76966 | valid_AIR: 0.76376 |  0:01:30s\n",
      "epoch 67 | loss: 0.21502 | train_AIR: 0.76217 | valid_AIR: 0.74771 |  0:01:31s\n",
      "epoch 68 | loss: 0.21436 | train_AIR: 0.76498 | valid_AIR: 0.74541 |  0:01:33s\n",
      "epoch 69 | loss: 0.21398 | train_AIR: 0.76779 | valid_AIR: 0.74541 |  0:01:34s\n",
      "epoch 70 | loss: 0.21312 | train_AIR: 0.76966 | valid_AIR: 0.74312 |  0:01:35s\n",
      "epoch 71 | loss: 0.21179 | train_AIR: 0.77247 | valid_AIR: 0.74541 |  0:01:37s\n",
      "epoch 72 | loss: 0.21013 | train_AIR: 0.76217 | valid_AIR: 0.76147 |  0:01:38s\n",
      "epoch 73 | loss: 0.21066 | train_AIR: 0.76124 | valid_AIR: 0.76147 |  0:01:39s\n",
      "epoch 74 | loss: 0.21185 | train_AIR: 0.75655 | valid_AIR: 0.74771 |  0:01:41s\n",
      "epoch 75 | loss: 0.21429 | train_AIR: 0.75843 | valid_AIR: 0.74771 |  0:01:42s\n",
      "epoch 76 | loss: 0.21346 | train_AIR: 0.75843 | valid_AIR: 0.74771 |  0:01:43s\n",
      "epoch 77 | loss: 0.21213 | train_AIR: 0.75936 | valid_AIR: 0.75    |  0:01:45s\n",
      "epoch 78 | loss: 0.21206 | train_AIR: 0.75468 | valid_AIR: 0.75    |  0:01:46s\n",
      "epoch 79 | loss: 0.21026 | train_AIR: 0.76873 | valid_AIR: 0.76835 |  0:01:47s\n",
      "epoch 80 | loss: 0.20912 | train_AIR: 0.7706  | valid_AIR: 0.77294 |  0:01:49s\n",
      "epoch 81 | loss: 0.20862 | train_AIR: 0.76404 | valid_AIR: 0.75917 |  0:01:50s\n",
      "epoch 82 | loss: 0.20896 | train_AIR: 0.75936 | valid_AIR: 0.74771 |  0:01:51s\n",
      "epoch 83 | loss: 0.20727 | train_AIR: 0.75094 | valid_AIR: 0.75688 |  0:01:52s\n",
      "epoch 84 | loss: 0.20686 | train_AIR: 0.76217 | valid_AIR: 0.75917 |  0:01:54s\n",
      "epoch 85 | loss: 0.20582 | train_AIR: 0.75936 | valid_AIR: 0.75688 |  0:01:55s\n",
      "epoch 86 | loss: 0.20561 | train_AIR: 0.75655 | valid_AIR: 0.75229 |  0:01:56s\n",
      "epoch 87 | loss: 0.20577 | train_AIR: 0.75749 | valid_AIR: 0.76376 |  0:01:58s\n",
      "epoch 88 | loss: 0.20567 | train_AIR: 0.75187 | valid_AIR: 0.75917 |  0:01:59s\n",
      "epoch 89 | loss: 0.20501 | train_AIR: 0.75655 | valid_AIR: 0.75459 |  0:02:00s\n",
      "epoch 90 | loss: 0.20534 | train_AIR: 0.76311 | valid_AIR: 0.75917 |  0:02:02s\n",
      "epoch 91 | loss: 0.2043  | train_AIR: 0.76217 | valid_AIR: 0.75    |  0:02:03s\n",
      "epoch 92 | loss: 0.20351 | train_AIR: 0.76404 | valid_AIR: 0.74312 |  0:02:04s\n",
      "epoch 93 | loss: 0.20254 | train_AIR: 0.76124 | valid_AIR: 0.75688 |  0:02:06s\n",
      "epoch 94 | loss: 0.20264 | train_AIR: 0.76124 | valid_AIR: 0.73853 |  0:02:07s\n",
      "epoch 95 | loss: 0.20217 | train_AIR: 0.75562 | valid_AIR: 0.74541 |  0:02:08s\n",
      "epoch 96 | loss: 0.20092 | train_AIR: 0.75468 | valid_AIR: 0.74771 |  0:02:10s\n",
      "epoch 97 | loss: 0.20049 | train_AIR: 0.75281 | valid_AIR: 0.76147 |  0:02:11s\n",
      "epoch 98 | loss: 0.2001  | train_AIR: 0.75375 | valid_AIR: 0.75459 |  0:02:12s\n",
      "epoch 99 | loss: 0.19945 | train_AIR: 0.75843 | valid_AIR: 0.75229 |  0:02:14s\n",
      "epoch 100| loss: 0.19975 | train_AIR: 0.75749 | valid_AIR: 0.75459 |  0:02:15s\n",
      "epoch 101| loss: 0.199   | train_AIR: 0.75843 | valid_AIR: 0.75688 |  0:02:16s\n",
      "epoch 102| loss: 0.19917 | train_AIR: 0.75843 | valid_AIR: 0.75688 |  0:02:18s\n",
      "epoch 103| loss: 0.1983  | train_AIR: 0.76217 | valid_AIR: 0.74541 |  0:02:19s\n",
      "epoch 104| loss: 0.19838 | train_AIR: 0.75094 | valid_AIR: 0.75459 |  0:02:20s\n",
      "epoch 105| loss: 0.1981  | train_AIR: 0.75281 | valid_AIR: 0.74771 |  0:02:22s\n",
      "epoch 106| loss: 0.19653 | train_AIR: 0.76311 | valid_AIR: 0.74771 |  0:02:23s\n",
      "\n",
      "Early stopping occurred at epoch 106 with best_epoch = 56 and best_valid_AIR = 0.77523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yaoching\\Desktop\\ICCV_Smart_cascade_code\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    # custom loss\n",
    "    loss_fn=TSP_Loss(lambda_CDC_weight = lambda_CDC_weight, Loss_Base_weight = Loss_Base_weight),\n",
    "    eval_metric=eval_metric,\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50,\n",
    "    batch_size=512, virtual_batch_size=128, #default:batch_size=1024, virtual_batch_size=128\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving weight of estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at c:\\Users\\yaoching\\Desktop\\ICCV_Smart_cascade_code\\estimator_weights\\weights_resnet18_resnet101\\TabNet_L-Overall_AIR_Seed_1.zip\n"
     ]
    }
   ],
   "source": [
    "#save tabnet model\n",
    "saved_filepath = clf.save_model(saving_weight_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0395687]]\n",
      "[[0.98425615]]\n",
      "[[0.9629064]]\n",
      "[[0.65394646]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(np.array([1.0,0.,0.,0.,0.,0.]+[0.0]*94).reshape(-1,100)))\n",
    "print(clf.predict(np.array([0.9,0.1,0.,0.,0.,0.]+[0.0]*94).reshape(-1,100)))\n",
    "print(clf.predict(np.array([0.8,0.2,0.,0.,0.,0.]+[0.0]*94).reshape(-1,100)))\n",
    "print(clf.predict(np.array([0.7,0.3,0.,0.,0.,0.]+[0.0]*94).reshape(-1,100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c9ec4ef67adeae2443ac3c3a3cebc4382116ad0366849b33d882866e4949921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
